P8105 Homework 3
================
Jiayi Shen
10/14/2018

Problem 1
=========

``` r
# tidying up BRFSS data
brfss_smart2010 = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% 
  mutate(response = as_factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"), ordered = TRUE))
```

``` r
# In 2002, what states were observed at 7 locations?
brfss_smart2010 %>% 
  filter(year == "2002") %>% 
  distinct(locationabbr, locationdesc) %>% 
  count(locationabbr) %>% 
  filter(n == 7)
```

    ## # A tibble: 3 x 2
    ##   locationabbr     n
    ##   <chr>        <int>
    ## 1 CT               7
    ## 2 FL               7
    ## 3 NC               7

In 2002, Connecticut, Florida and North Carolina were observed at 7 locations.

``` r
# create a relevant dataframe for spaghetti plot
spaghetti_df = 
brfss_smart2010 %>% 
  group_by(locationabbr, year) %>%
  distinct(locationdesc) %>% 
  count(locationabbr)

# a line plot that shows the number of obs in each state from 2002 to 2010. 
ggplot(spaghetti_df, aes(x = year, y = n, color = locationabbr)) +
  geom_line() +
  geom_point() +
  labs(x = "Year", 
       y = "Num. of locations",
       title = "Number of locations in each state from 2002 to 2010") +
   theme(legend.position = "bottom", 
         legend.text = element_text(size = 9),
         axis.title=element_text(size = 9,face = "bold")) +
  guides(colour = guide_legend(nrow = 7))
```

![](p8105_hw3_js5354_files/figure-markdown_github/spaghetti%20plot-1.png)

The line plot above shows changes in the number of locations of each state in the BRFSS data from 2002 to 2010. Lines are colored by which state they represent. It is noticable that FL had more than 40 locations submitting their responses in 2007 and 2010, which is about twice the number of locations in the other states in these two years.

``` r
#First filter brfss data accordingly, and then calculate the mean and sd.
brfss_smart2010 %>% 
  filter(year %in% c("2002", "2006", "2010")) %>% 
  filter(locationabbr == "NY", response == "Excellent") %>% 
  group_by(year) %>% 
  summarize(mean_NY = mean(data_value), 
            sd_NY = sd(data_value)) %>% 
  knitr::kable(digits = 1)
```

|  year|  mean\_NY|  sd\_NY|
|-----:|---------:|-------:|
|  2002|      24.0|     4.5|
|  2006|      22.5|     4.0|
|  2010|      22.7|     3.6|

From 2002 to 2006, the average proportion of excellent response in all locations across NY dropped from 24.0% to 22.5%. And from 2006 to 2010, this proportion stayed relatively flat, around 22.5%. The variance between each location across the NY state decreased from 2002 to 2006 and then to 2010.

``` r
# create a relevant dataframe for plotting
brfss_state_avg = 
brfss_smart2010 %>% 
  group_by(year, locationabbr, response) %>% 
  summarize(avg_prop = mean(data_value))

# plot brfss_state_avg
ggplot(brfss_state_avg, aes(x = year, y = avg_prop, color = locationabbr))+
  geom_line()+
  labs(x = "Year",
       y = "Average percentage of each response",
       title = "The state-level average percentage \nof each response from 2002 to 2010") +
  facet_grid(. ~ response) +
  theme(legend.position = "bottom", 
        legend.text = element_text(size = 9),
        axis.title = element_text(size = 10,face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  guides(colour = guide_legend(nrow = 7))
```

    ## Warning: Removed 1 rows containing missing values (geom_path).

![](p8105_hw3_js5354_files/figure-markdown_github/state%20level%20average%20of%20each%20response-1.png)

In general, the average proportion of responding either "excellent" or "very good" or "good" is greater than those of responding "fair" or "poor".

Problem 2
=========

``` r
# load instacart data and have a quick look 
data(instacart, package = "p8105.datasets") 
head(instacart)
```

    ## # A tibble: 6 x 15
    ##   order_id product_id add_to_cart_ordâ€¦ reordered user_id eval_set
    ##      <int>      <int>            <int>     <int>   <int> <chr>   
    ## 1        1      49302                1         1  112108 train   
    ## 2        1      11109                2         1  112108 train   
    ## 3        1      10246                3         0  112108 train   
    ## 4        1      49683                4         0  112108 train   
    ## 5        1      43633                5         1  112108 train   
    ## 6        1      13176                6         0  112108 train   
    ## # ... with 9 more variables: order_number <int>, order_dow <int>,
    ## #   order_hour_of_day <int>, days_since_prior_order <int>,
    ## #   product_name <chr>, aisle_id <int>, department_id <int>, aisle <chr>,
    ## #   department <chr>

The `instacart` dataset contains 1384617 observations and 15 variables, storing information related to orders placed on the instacart app. The dataset not only includes numeric variables, such as order ID and user ID, but also character variables like descriptive product names and department names, and a logical variable `reordered` stating whether the order was placed first time or not.

Taking `order_id` =1 placed by `user_id` = 112108 as an example, this specific order was consist of 8 items with their `product_id`, `product_name` and the order in which each item was added to the cart (`add_to_cart_order`) stated. If `reordered` =1, it means that the particular item was not the first time that customer purchased. `order_number` refers to the sequence number of this order that the user placed. In this case, the user purchased four of everything. `order_dow` = 4 and `order_hour_of_day` =12 means this order was placed on a Thursday on 12pm. It had been 30 days (`days_since_prior_order` = 30) since this user placed his/her last order on instacart. Information related to the location of each product in the supermarket is specified in `aisle_id`, `aisle`, `department_id` and `department`.

``` r
# The number of distinct aisles
distinct(instacart, aisle)
```

    ## # A tibble: 134 x 1
    ##    aisle                        
    ##    <chr>                        
    ##  1 yogurt                       
    ##  2 other creams cheeses         
    ##  3 fresh vegetables             
    ##  4 canned meat seafood          
    ##  5 fresh fruits                 
    ##  6 packaged cheese              
    ##  7 specialty cheeses            
    ##  8 water seltzer sparkling water
    ##  9 cream                        
    ## 10 packaged vegetables fruits   
    ## # ... with 124 more rows

``` r
# Frequency of each aisle
instacart %>% 
  group_by(aisle) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # ... with 124 more rows

There are in total 134 aisles. Among these, fresh fruits and fresh vegetables are the two aisles from which most items are ordered. The frequency of fresh fruits/vegetables appeared in this dataset almost double the frequency of the third most popular aisle, packaged vegetables fruits.

``` r
# create a relevant dataframe for further plotting
aisle_freq = 
instacart %>% 
  group_by(aisle) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n))

# create a function to plot aisles with item number within a specific interval.
plot_aisle = function (lower_lim, upper_lim){

  ggplot(filter(aisle_freq, n >= lower_lim & n < upper_lim), aes(x = reorder(aisle, -n), y = n)) +
  geom_bar(stat = "identity")  +
  geom_text(aes(label = n), size =1, position = position_dodge(width = 0.9), vjust = -0.25) +
  scale_y_continuous(name = "Number of items ordered") +
  scale_x_discrete(name = "Asile" ) +
  coord_cartesian(ylim = c(lower_lim*0.95, upper_lim*1.05)) +
  labs(title = paste("Aisles with more than",lower_lim, "\nand less than", upper_lim, "items ordered")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title = element_text(size = 10,face = "bold"),
        plot.margin = margin(0, 1.5, 0, 1, "cm"))
}

aisle_p1 = plot_aisle (0, 1000)
aisle_p2 = plot_aisle (1000, 2000)
aisle_p3 = plot_aisle (2000, 6000)
aisle_p4 = plot_aisle (6000, 13000)
aisle_p5 = plot_aisle (13000, 151000)

aisle_p5  
```

![](p8105_hw3_js5354_files/figure-markdown_github/plotting%20n.%20of%20items%20in%20each%20aisle-1.png)

``` r
aisle_p4  
```

![](p8105_hw3_js5354_files/figure-markdown_github/plotting%20n.%20of%20items%20in%20each%20aisle-2.png)

``` r
aisle_p3  
```

![](p8105_hw3_js5354_files/figure-markdown_github/plotting%20n.%20of%20items%20in%20each%20aisle-3.png)

``` r
aisle_p2 
```

![](p8105_hw3_js5354_files/figure-markdown_github/plotting%20n.%20of%20items%20in%20each%20aisle-4.png)

``` r
aisle_p1 
```

![](p8105_hw3_js5354_files/figure-markdown_github/plotting%20n.%20of%20items%20in%20each%20aisle-5.png)

The 5 bar charts above demonstrate the number of items ordered in the 134 aisles in `instacart` dataset. Bars are ordered descendingly from left to right in every subplot. The range of items ordered across different asiles is very broad, from less than 300 to over 150,000.

``` r
# make a table showing the most popular item in aisles "baking ingredients", "dog food care", "packaged vegetables fruits"

instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(item_ranking = min_rank(desc(n))) %>% 
  filter(item_ranking == 1) %>% 
  select(-item_ranking) %>% 
  knitr::kable()
```

| aisle                      | product\_name                                 |     n|
|:---------------------------|:----------------------------------------------|-----:|
| baking ingredients         | Light Brown Sugar                             |   499|
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |    30|
| packaged vegetables fruits | Organic Baby Spinach                          |  9784|

The most popular item in aisles "baking ingredients", "dog food care", "packaged vegetables fruits" are "Light Brown Sugar", "Snack Sticks Chicken & Rice Recipe Dog Treats", and "Organic Baby Spinach" respectively. A total of 9784 portions of "Organic Baby Spinach" were purchased in this data.

``` r
# filter product_name and then make adjustments to date/time variables accordingly.
instacart %>% 
  filter(product_name %in%  c("Pink Lady Apples","Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hof = mean(order_hour_of_day)) %>% 
  mutate(mean_hof = paste(floor(mean_hof), round((mean_hof-floor(mean_hof))*60), sep=":")) %>% 
  spread(key = order_dow, value = mean_hof) %>% 
  rename('Sun' = '0', 'Mon' = '1', 'Tue' = '2', 
         'Wed' = '3', 'Thu' = '4', 'Fri' = '5', 'Sat' = '6') %>% 
  knitr::kable() 
```

| product\_name    | Sun   | Mon   | Tue   | Wed   | Thu   | Fri   | Sat   |
|:-----------------|:------|:------|:------|:------|:------|:------|:------|
| Coffee Ice Cream | 13:46 | 14:19 | 15:23 | 15:19 | 15:13 | 12:16 | 13:50 |
| Pink Lady Apples | 13:26 | 11:22 | 11:42 | 14:15 | 11:33 | 12:47 | 11:56 |

The table above lists the average hour of day when pink lady apples and coffee ice cream are ordered, based on the `instacart` data. In general, the two items are mainly ordered in noon and early afternoon, in a window between 11:00 to 14:30. Differences in the mean hour of day among different weekdays are subtle.

Problem 3
=========

``` r
data(ny_noaa, package = "p8105.datasets")
head(ny_noaa)
```

    ## # A tibble: 6 x 7
    ##   id          date        prcp  snow  snwd tmax  tmin 
    ##   <chr>       <date>     <int> <int> <int> <chr> <chr>
    ## 1 US1NYAB0001 2007-11-01    NA    NA    NA <NA>  <NA> 
    ## 2 US1NYAB0001 2007-11-02    NA    NA    NA <NA>  <NA> 
    ## 3 US1NYAB0001 2007-11-03    NA    NA    NA <NA>  <NA> 
    ## 4 US1NYAB0001 2007-11-04    NA    NA    NA <NA>  <NA> 
    ## 5 US1NYAB0001 2007-11-05    NA    NA    NA <NA>  <NA> 
    ## 6 US1NYAB0001 2007-11-06    NA    NA    NA <NA>  <NA>

The `ny_noaa` dataset contains 2595176 records and 7 variables relevant to national-wise daily weather data. The records are reported from 747 distinct stations, from 1981-01-01 to 2010-12-31. Precipitation (in tenths of mm), snowfall (in mm), snow depth (in mm) and maximun / minimum temperature (in tenths of degree C) are the variables included in this dataset.

Out of the total 2595176 records, there are 0 % missing values in the variable `tmax_dC` and `tmin_dC`; 15% in `snow_mm`; 23% in `snwd_mm`.

The relatively large proportion of missing values may reduce reliability of statistics generated based on this dataset. Limited number of observations on tmax/tmin during earlier years, due to the large quantity of their missing values, may render the samples in this dataset being less representative of the overall weather pattern in NY.

``` r
#separate the date
#include reasonable units for temp, prcp and snowfall
ny_noaa =
ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") 

# change the type of tmax and tmin
ny_noaa$tmax = as.numeric(unclass(ny_noaa$tmax))
ny_noaa$tmin = as.numeric(unclass(ny_noaa$tmin))
ny_noaa$year = as.numeric(unclass(ny_noaa$year))
ny_noaa$month = as.numeric(unclass(ny_noaa$month))
ny_noaa$day = as.numeric(unclass(ny_noaa$day))

#mutate the value of varialbes so that they are in agreement with reasonable units
ny_noaa =
ny_noaa %>% 
  mutate(tmax = tmax *0.1, tmin = tmin*0.1, prcp = prcp*0.1) %>% 
  rename (prcp_mm = prcp, snow_mm = snow, snwd_mm = snwd, tmax_dC = tmax, tmin_dC = tmin)

#get the mode of snallfall values
getmode <- function(input) {
   uniqv <- unique(input)
   uniqv[which.max(tabulate(match(input, uniqv)))]
}
getmode(ny_noaa$snow_mm)
```

    ## [1] 0

The most commmonly observed value of snowfall is 0mm among all New York state weather stations from January 1, 1981 through December 31, 2010. The reason is probably that the snowing season in New York state only takes place for a relatively short duration of time throughout a year.

``` r
# first working on the avg temp in JAN
avg_temp_jan = 
ny_noaa %>% 
  na.omit(cols= c("tmax_dC", "tmin_dC")) %>% 
  filter(month == 1) %>% 
  group_by(id, year) %>% 
  summarize(avg_temp = mean(tmax_dC)) 

p_JAN = 
ggplot (avg_temp_jan, aes(x = year, y = avg_temp, color = id)) +
  geom_line() +
  labs (title = "Average Temperature (degrees C) in January \neach year of each station, 1980-2010",
        x = "Year",
        y = "Temperature (degrees C)") +
  theme(legend.position="none")

# then working on the avg temp in JUL
avg_temp_jul = 
ny_noaa %>% 
  na.omit(cols= c("tmax_dC", "tmin_dC")) %>% 
  filter(month == 7) %>% 
  group_by(id, year) %>% 
  summarize(avg_temp = mean(tmax_dC) ) 

p_JUL = 
ggplot (avg_temp_jul, aes(x = year, y = avg_temp, color = id)) +
  geom_line() +
  labs (title = "Average Temperature (degrees C) in July \neach year of each station, 1980-2010",
        x = "Year",
        y = "Temperature (degrees C)") +
  theme(legend.position="none")

p_JAN / p_JUL
```

![](p8105_hw3_js5354_files/figure-markdown_github/plotting%20avg%20temp%20in%20JAN/JUL-1.png)

``` r
# Finding out the outliers
filter (avg_temp_jan, avg_temp == min(avg_temp_jan$avg_temp))
```

    ## # A tibble: 1 x 3
    ## # Groups:   id [1]
    ##   id           year avg_temp
    ##   <chr>       <dbl>    <dbl>
    ## 1 USC00303889  1982    -16.6

``` r
filter (avg_temp_jul, avg_temp == min(avg_temp_jul$avg_temp))
```

    ## # A tibble: 1 x 3
    ## # Groups:   id [1]
    ##   id           year avg_temp
    ##   <chr>       <dbl>    <dbl>
    ## 1 USC00308962  1988     14.0

In general, the average temperature in January/July in each station of NY state follows relatively similar pattern across 1980 to 2010. However, there are two noticable outliers: the average temperature of -20.2 degrees C in January 1996 and the one of 14.0 degrees C in July 2007.

``` r
# plotting tmax vs tmin using geom_bin2d
tmax_tmin_plot = 
ny_noaa %>% 
  na.omit(cols= c("tmax_dC", "tmin_dC")) %>% 
  ggplot(aes(x = tmin_dC, y = tmax_dC)) +
  labs(title = "Max Temperature VS Min Temperature \nfor All NY NOAA Data",
       x = "Min Temperature (degrees C)",
       y = "Max Temperature (degrees C)") +
  geom_bin2d()
```

``` r
# plott distribution of snowfall values with geom_density_ridges
snowfall_plot = 
ny_noaa  %>% 
  na.omit(cols = "snow_mm") %>% 
  filter(snow_mm > 0 & snow_mm <100) %>% 
  ggplot(aes(x = snow_mm, y = as.character(year))) +
  geom_density_ridges(adjust = 3) +
  labs (title = "Snowfall Distribution in NY State \nfrom 1980 to 2010",
        x = "Snowfall (mm)",
        y = "Year") 
```

    ## Warning: Ignoring unknown parameters: adjust

``` r
# make a two panel plot
tmax_tmin_plot / snowfall_plot
```

    ## Picking joint bandwidth of 4.24

![](p8105_hw3_js5354_files/figure-markdown_github/distribution%20of%20snowfall%20values-1.png)

As shown in the top panel showing a heatmap of maximum temperature versus minimum temperature for the entire `ny_noaa` dataset, the relationship between max temperature and min temperature seems to be positively linear-dependent. In other words, the max temperature increases as the min temperature increases. In addition, the most observed max & min temperature records are colored in light blue while darker colors indicate less commonly being being observed.

The bottom panel is a density plot showing the distribution of snowfall (ranged from 0 to 100 mm) in NY from 1981 to 2010. Across these 30 years, the distribution looks very much similar, with most records between 0 to 40 mm and two smaller peaks at about 45 mm and 75 mm respectively.
