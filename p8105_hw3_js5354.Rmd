---
title: "P8105 Homework 3"
author: Jiayi Shen
date: 10/14/2018
output: github_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      dpi = 300)
library(tidyverse)
library(ggridges)
library(patchwork)
library(grid)
```


# Problem 1
```{r load BRFSS data, include = FALSE}
# install.packages("devtools")
devtools::install_github("p8105/p8105.datasets")

#load BRFSS data
library(p8105.datasets)
data(brfss_smart2010, package = "p8105.datasets")
```

```{r clean up BRFSS}
# tidying up BRFSS data
brfss_smart2010 = brfss_smart2010 %>% 
  janitor::clean_names() %>% 
  filter(topic == "Overall Health") %>% 
  filter(response %in% c("Excellent", "Very good", "Good", "Fair", "Poor")) %>% 
  mutate(response = as_factor(response, levels = c("Excellent", "Very good", "Good", "Fair", "Poor"), ordered = TRUE))
  
```


```{r questions on BRFSS}
# In 2002, what states were observed at 7 locations?
brfss_smart2010 %>% 
  filter(year == "2002") %>% 
  distinct(locationabbr, locationdesc) %>% 
  count(locationabbr) %>% 
  filter(n == 7)
```

  In 2002, Connecticut, Florida and North Carolina were observed at 7 locations.  
  
```{r spaghetti plot, fig.width = 6, fig.height = 8, warning = FALSE}
# create a relevant dataframe for spaghetti plot
spaghetti_df = 
brfss_smart2010 %>% 
  group_by(locationabbr, year) %>%
  distinct(locationdesc) %>% 
  count(locationabbr)

# a line plot that shows the number of obs in each state from 2002 to 2010. 
ggplot(spaghetti_df, aes(x = year, y = n, color = locationabbr)) +
  geom_line() +
  geom_point() +
  labs(x = "Year", 
       y = "Num. of locations",
       title = "Number of locations in each state from 2002 to 2010") +
   theme(legend.position = "bottom", 
         legend.text = element_text(size = 9),
         axis.title=element_text(size = 9,face = "bold")) +
  guides(colour = guide_legend(nrow = 7))
```
    
  The line plot above shows changes in the number of locations of each state in the BRFSS data from 2002 to 2010. Lines are colored by which state they represent. It is noticable that FL had more than 40 locations submitting their responses in 2007 and 2010, which is about twice the number of locations in the other states in these two years. 
  
```{r table of excellent proportion in NY}

#First filter brfss data accordingly, and then calculate the mean and sd.
brfss_smart2010 %>% 
  filter(year %in% c("2002", "2006", "2010")) %>% 
  filter(locationabbr == "NY", response == "Excellent") %>% 
  group_by(year) %>% 
  summarize(mean_NY = mean(data_value), 
            sd_NY = sd(data_value)) %>% 
  knitr::kable(digits = 1)

```

  From 2002 to 2006, the average proportion of excellent response in all locations across NY dropped from 24.0% to 22.5%. And from 2006 to 2010, this proportion stayed relatively flat, around 22.5%. The variance between each location across the NY state decreased from 2002 to 2006 and then to 2010.  
  
  

```{r state level average of each response, fig.width = 6, fig.height = 8}
# create a relevant dataframe for plotting
brfss_state_avg = 
brfss_smart2010 %>% 
  group_by(year, locationabbr, response) %>% 
  summarize(avg_prop = mean(data_value))

# plot brfss_state_avg
ggplot(brfss_state_avg, aes(x = year, y = avg_prop, color = locationabbr))+
  geom_line()+
  labs(x = "Year",
       y = "Average percentage of each response",
       title = "The state-level average percentage \nof each response from 2002 to 2010") +
  facet_grid(. ~ response) +
  theme(legend.position = "bottom", 
        legend.text = element_text(size = 9),
        axis.title = element_text(size = 10,face = "bold"),
        axis.text.x = element_text(angle = 45, hjust = 1)) +
  guides(colour = guide_legend(nrow = 7))
```


  In general, the average proportion of responding either "excellent" or "very good" or "good" is greater than those of responding "fair" or "poor".   
  

#Problem 2
```{r load instacart data}
# load instacart data and have a quick look 
data(instacart, package = "p8105.datasets") 
head(instacart)
```
    
  The `instacart` dataset contains `r dim(instacart)[1]` observations and `r dim(instacart)[2]` variables, storing information related to orders placed on the instacart app. The dataset not only includes numeric variables, such as order ID and user ID, but also character variables like descriptive product names and department names, and a logical variable `reordered` stating whether the order was placed first time or not.  
  
  Taking `order_id` =1 placed by `user_id` = 112108 as an example, this specific order was consist of 8 items with their `product_id`, `product_name` and the order in which each item was added to the cart (`add_to_cart_order`) stated. If `reordered` =1, it means that the particular item was not the first time that customer purchased.  `order_number` refers to the sequence number of this order that the user placed. In this case, the user purchased four of everything. `order_dow` = 4 and `order_hour_of_day` =12 means this order was placed on a Thursday on 12pm. It had been 30 days (`days_since_prior_order` = 30) since this user placed his/her last order on instacart. Information related to the location of each product in the supermarket is specified in `aisle_id`, `aisle`, `department_id` and `department`.


```{r distinct aisles and their frequency}
# The number of distinct aisles
distinct(instacart, aisle)

# Frequency of each aisle
instacart %>% 
  group_by(aisle) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n))
```
  
  There are in total 134 aisles. Among these, fresh fruits and fresh vegetables are the two aisles from which most items are ordered. The frequency of fresh fruits/vegetables appeared in this dataset almost double the frequency of the third most popular aisle, packaged vegetables fruits. 

```{r plotting n. of items in each aisle, fig.width = 6, fig.height = 4}
# create a relevant dataframe for further plotting
aisle_freq = 
instacart %>% 
  group_by(aisle) %>% 
  summarize(n = n()) %>% 
  arrange(desc(n))

# create a function to plot aisles with item number within a specific interval.
plot_aisle = function (lower_lim, upper_lim){

  ggplot(filter(aisle_freq, n >= lower_lim & n < upper_lim), aes(x = reorder(aisle, -n), y = n)) +
  geom_bar(stat = "identity")  +
  geom_text(aes(label = n), size =1, position = position_dodge(width = 0.9), vjust = -0.25) +
  scale_y_continuous(name = "Number of items ordered") +
  scale_x_discrete(name = "Asile" ) +
  coord_cartesian(ylim = c(lower_lim*0.95, upper_lim*1.05)) +
  labs(title = paste("Aisles with more than",lower_lim, "\nand less than", upper_lim, "items ordered")) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        axis.title = element_text(size = 10,face = "bold"),
        plot.margin = margin(0, 1.5, 0, 1, "cm"))
}

aisle_p1 = plot_aisle (0, 1000)
aisle_p2 = plot_aisle (1000, 2000)
aisle_p3 = plot_aisle (2000, 6000)
aisle_p4 = plot_aisle (6000, 13000)
aisle_p5 = plot_aisle (13000, 151000)

aisle_p5  
aisle_p4  
aisle_p3  
aisle_p2 
aisle_p1 

```

  The 5 bar charts above demonstrate the number of items ordered in the 134 aisles in `instacart` dataset. Bars are ordered descendingly from left to right in every subplot. The range of items ordered across different asiles is very broad, from less than 300 to over 150,000. 


```{r table: the most popular item in 3 aisles}
# make a table showing the most popular item in aisles "baking ingredients", "dog food care", "packaged vegetables fruits"

instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(item_ranking = min_rank(desc(n))) %>% 
  filter(item_ranking == 1) %>% 
  select(-item_ranking) %>% 
  knitr::kable()
  
```


  The most popular item in aisles "baking ingredients", "dog food care", "packaged vegetables fruits" are "Light Brown Sugar", "Snack Sticks Chicken & Rice Recipe Dog Treats", and "Organic Baby Spinach" respectively. A total of 9784 portions of "Organic Baby Spinach" were purchased in this data. 


```{r table: the mean hour of the day}
# filter product_name and then make adjustments to date/time variables accordingly.
instacart %>% 
  filter(product_name %in%  c("Pink Lady Apples","Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hof = mean(order_hour_of_day)) %>% 
  mutate(mean_hof = paste(floor(mean_hof), round((mean_hof-floor(mean_hof))*60), sep=":")) %>% 
  spread(key = order_dow, value = mean_hof) %>% 
  rename('Sun' = '0', 'Mon' = '1', 'Tue' = '2', 
         'Wed' = '3', 'Thu' = '4', 'Fri' = '5', 'Sat' = '6') %>% 
  knitr::kable() 

```


  The table above lists the average hour of day when pink lady apples and coffee ice cream are ordered, based on the `instacart` data. In general, the two items are mainly ordered in noon and early afternoon, in a window between 11:00 to 14:30. Differences in the mean hour of day among different weekdays are subtle. 

#Problem 3

```{r load ny_noaa data}
data(ny_noaa, package = "p8105.datasets")
head(ny_noaa)

```
  
  The `ny_noaa` dataset contains `r dim(ny_noaa)[1]` records and `r dim(ny_noaa)[2]` variables relevant to national-wise daily weather data. The records are reported from `r nrow(distinct(ny_noaa,id))` distinct stations, from `r range(ny_noaa$date)[1]` to `r range(ny_noaa$date)[2]`. Precipitation (in tenths of mm), snowfall (in mm), snow depth (in mm) and maximun / minimum temperature (in tenths of degree C) are the variables included in this dataset.   
  
  Out of the total `r dim(ny_noaa)[1]` records, there are `r round(sum(is.na(ny_noaa$tmax_dC)) / nrow(ny_noaa)*100)` % missing values in the variable `tmax_dC` and `tmin_dC`; `r round(sum(is.na(ny_noaa$snow)) / nrow(ny_noaa)*100)`% in `snow_mm`; `r round(sum(is.na(ny_noaa$snwd)) / nrow(ny_noaa)*100)`% in `snwd_mm`.   
  
  The relatively large proportion of missing values may reduce reliability of statistics generated based on this dataset. Limited number of observations on tmax/tmin during earlier years, due to the large quantity of their missing values, may render the samples in this dataset being less representative of the overall weather pattern in NY. 
  
```{r cleaning of ny_noaa}
#separate the date
#include reasonable units for temp, prcp and snowfall
ny_noaa =
ny_noaa %>% 
  separate(date, into = c("year", "month", "day"), sep = "-") 

# change the type of tmax and tmin
ny_noaa$tmax = as.numeric(unclass(ny_noaa$tmax))
ny_noaa$tmin = as.numeric(unclass(ny_noaa$tmin))
ny_noaa$year = as.numeric(unclass(ny_noaa$year))
ny_noaa$month = as.numeric(unclass(ny_noaa$month))
ny_noaa$day = as.numeric(unclass(ny_noaa$day))

#mutate the value of varialbes so that they are in agreement with reasonable units
ny_noaa =
ny_noaa %>% 
  mutate(tmax = tmax *0.1, tmin = tmin*0.1, prcp = prcp*0.1) %>% 
  rename (prcp_mm = prcp, snow_mm = snow, snwd_mm = snwd, tmax_dC = tmax, tmin_dC = tmin)

#get the mode of snallfall values
getmode <- function(input) {
   uniqv <- unique(input)
   uniqv[which.max(tabulate(match(input, uniqv)))]
}
getmode(ny_noaa$snow_mm)
```

  The most commmonly observed value of snowfall is 0mm among all New York state weather stations from January 1, 1981 through December 31, 2010. The reason is probably that the snowing season in New York state only takes place for a relatively short duration of time throughout a year.

```{r plotting avg temp in JAN/JUL}
# first working on the avg temp in JAN
avg_temp_jan = 
ny_noaa %>% 
  na.omit(cols= c("tmax_dC", "tmin_dC")) %>% 
  filter(month == 1) %>% 
  group_by(id, year) %>% 
  summarize(avg_temp = mean(tmax_dC)) 

p_JAN = 
ggplot (avg_temp_jan, aes(x = year, y = avg_temp, color = id)) +
  geom_line() +
  labs (title = "Average Temperature (degrees C) in January \neach year of each station, 1980-2010",
        x = "Year",
        y = "Temperature (degrees C)") +
  theme(legend.position="none")

# then working on the avg temp in JUL
avg_temp_jul = 
ny_noaa %>% 
  na.omit(cols= c("tmax_dC", "tmin_dC")) %>% 
  filter(month == 7) %>% 
  group_by(id, year) %>% 
  summarize(avg_temp = mean(tmax_dC) ) 

p_JUL = 
ggplot (avg_temp_jul, aes(x = year, y = avg_temp, color = id)) +
  geom_line() +
  labs (title = "Average Temperature (degrees C) in July \neach year of each station, 1980-2010",
        x = "Year",
        y = "Temperature (degrees C)") +
  theme(legend.position="none")

p_JAN / p_JUL

# Finding out the outliers
filter (avg_temp_jan, avg_temp == min(avg_temp_jan$avg_temp))
filter (avg_temp_jul, avg_temp == min(avg_temp_jul$avg_temp))
```

  In general, the average temperature in January/July in each station of NY state follows relatively similar pattern across 1980 to 2010. However, there are two noticable outliers: the average temperature of -20.2 degrees C in January 1996 and the one of 14.0 degrees C in July 2007. 

```{r plotting tmax vs tmin, warning = FALSE}
# plotting tmax vs tmin using geom_bin2d
tmax_tmin_plot = 
ny_noaa %>% 
  na.omit(cols= c("tmax_dC", "tmin_dC")) %>% 
  ggplot(aes(x = tmin_dC, y = tmax_dC)) +
  labs(title = "Max Temperature VS Min Temperature \nfor All NY NOAA Data",
       x = "Min Temperature (degrees C)",
       y = "Max Temperature (degrees C)") +
  geom_bin2d()
```

```{r distribution of snowfall values, fig.width = 6, fig.height = 9}
# plott distribution of snowfall values with geom_density_ridges
snowfall_plot = 
ny_noaa  %>% 
  na.omit(cols = "snow_mm") %>% 
  filter(snow_mm > 0 & snow_mm <100) %>% 
  ggplot(aes(x = snow_mm, y = as.character(year))) +
  geom_density_ridges(adjust = 3) +
  labs (title = "Snowfall Distribution in NY State \nfrom 1980 to 2010",
        x = "Snowfall (mm)",
        y = "Year") 

# make a two panel plot
tmax_tmin_plot / snowfall_plot
```

  As shown in the top panel showing a heatmap of maximum temperature versus minimum temperature for the entire `ny_noaa` dataset, the relationship between max temperature and min temperature seems to be positively linear-dependent. In other words, the max temperature increases as the min temperature increases. In addition, the most observed max & min temperature records are colored in light blue while darker colors indicate less commonly being being observed. 
  
  The bottom panel is a density plot showing the distribution of snowfall (ranged from 0 to 100 mm) in NY from 1981 to 2010. Across these 30 years, the distribution looks very much similar, with most records between 0 to 40 mm and two smaller peaks at about 45 mm and 75 mm respectively. 

